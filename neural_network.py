# -*- coding: utf-8 -*-
"""Neural-Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1joDh1lyeN4rgqb_H8UiGKNe4Or2-gpUO

# Dog breed Classification with Neural Network

End-to-end multi-class image classifier using TensorFlow 2.0 and TensorFlow Hub.

## 1. Problem

Identifying the breed of a dog given an image of a dog.

## 2. Data

Data from https://www.kaggle.com/competitions/dog-breed-identification/data.

## 3. Evaluation

The evaluation is a file with prediction probabilities for each dog breed of each test image.
www.kaggle.com/competitions/dog-breed-identification/overview/evaluation

## 4. Features

Information about the data:
* Images (unstructured data).
* There are 120 breeds of dogs (different).
* There are 10,000+ images in the training and test sets
* Training images have labels, it is the breeds

# Import libraries
"""

#!pip install tensorflow==2.15.0 tensorflow-hub keras==2.15.0

import tensorflow as tf
import tensorflow_hub as hub
print("TF version:", tf.__version__)
print("TF Hub version:", hub.__version__)

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from matplotlib.pyplot import imread
import matplotlib.pyplot as plt
import datetime

# Check for GPU availability
print("GPU", "available" if tf.config.list_physical_devices("GPU") else "not available")

"""# Getting the data ready"""

# Checkout the labels of our data
labels_csv = pd.read_csv("/content/drive/MyDrive/Dog vision/labels.csv")
print(labels_csv.describe())
print(labels_csv.head())

# How many images are there of each breed?
labels_csv["breed"].value_counts().plot.bar(figsize=(20, 10));

"""### Getting images and their labels"""

# Create pathnames from image ID's
filenames = ["/content/drive/MyDrive/Dog vision/train/" + fname + ".jpg" for fname in labels_csv["id"]]

filenames[:10]

# Check whether number of filenames matches number of actual image files
if len(os.listdir("/content/drive/MyDrive/Dog vision/train")) == len(filenames):
  print("Lenght is match")
else:
  print("Lenght doesn't match")

# Get labels
labels = labels_csv["breed"].to_numpy()
labels

# Check number of labels and number of filenames
if len(labels) == len(filenames):
  print("Lenght is match")
else:
  print("Lenght doesn't match")

# Unique labels
unique_breeds = np.unique(labels)
unique_breeds

# Turn every label into a boolean array
boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

len(boolean_labels)

# Turning boolean array into integers
print(labels[0]) # original label
print(np.where(unique_breeds == labels[0])) #index where label occurs
print(boolean_labels[0].argmax()) # index where label occurs in boolean array
print(boolean_labels[0].astype(int)) # there will be a 1 where the sample label occurs

filenames[:10]

"""### Create validation set"""

# Setup X and y variables
X = filenames
y = boolean_labels

"""Start with 1000 images, 10.000+ is too many"""

# Set number of images to use
num_images = 1000 #@param {type:"slider", min:1000, max:10000, step:1000}

# Split data into train and validation of total size num_images
X_train, X_val, y_train, y_val = train_test_split(X[:num_images],
                                                    y[:num_images],
                                                    test_size=0.2,
                                                    random_state=0)

# Data shape check
len(X_train), len(y_train), len(X_val), len(y_val)

"""## Preprocessing Images (tuning images into tensors)

1. Take an image filepath as input
2. Use TensorFlow to read the file and save it to a variable, `image`
3. Turn the `image` into Tensors
4. Normalize the image (convert color channel values from 0-255 to 0-1)
5. Resize the `image` to be a shape of (224,224), This need to the model, it is the input
6. Return the modified `image`

Check what importing in neural network
"""

# Convert image to numpy array
image = imread(filenames[0])
image.shape # height, width, colour channel (RGB, red green blue)

image[:2]

# turn image into a tensor
tf.constant(image)[:2]

# Define image size
img_size = 224

# Create a function for preprocessing images
def process_image(image_path, img_size=img_size):
  # Turn the image into a tensor
  image = tf.io.read_file(image_path) # read file
  # turn the jpeg image into numberical Tensor with 3 color channels (Red, Green, Blue)
  # this command do the same thing then tf.constant
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert the color channel values from 0-255 to 0-1 values
  # its normalization, so that there are no outliers
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resize the image to (224,224)
  image = tf.image.resize(image, size=[img_size, img_size])
  return image

"""## Turning data into batches

Why turn the data into batches?

When trying to process 10,000+ images in one go, they all might not fit into memory.

This is why use 32 (this is the batch size) images at a time.

In order to use TensorFlow effectively, the data need in this form:
`(image, label)`
"""

# Create a function to return a tuple (image, label)
def get_image_label(image_path, label):
  image = process_image(image_path)
  return image, label

"""A way to turn the data into tuples of Tensors in the form: `(image, label)`. Now make a function to turn all data into X and y"""

# Define the batch size, 32 is the start
batch_size = 32

# Create a function to turn data into batches
def create_data_batches(X, y=None, batch_size=batch_size, valid_data=False, test_data=False):
  """
  Creates batches of data out of image (X) and label (y) pairs.
  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.
  Also accepts test data as input (no labels).
  """
  # If the data is a test dataset, it doesn't have labels
  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths
    data_batch = data.map(process_image).batch(batch_size)
    return data_batch

  # if the data is a valid dataset, it doesn't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths
                                               tf.constant(y))) # label
    data_batch = data.map(get_image_label).batch(batch_size)
    return data_batch

  else:
    print("Creating training data batches...")
    # Turn filepaths and labels into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths
                                               tf.constant(y))) # label
    # Shuffling the data before mapping image processor function is faster than shuffling images
    data = data.shuffle(buffer_size=len(X))
    # Create (image, label) tuples (this also turns the image path into a preprocessed image)
    data = data.map(get_image_label)
    # Turn the data into batches
    data_batch = data.batch(batch_size)
  return data_batch

# Create training and validation data batches
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

# Check out the different attributes of the data batches
train_data.element_spec, val_data.element_spec

"""## Visualizing Data Batches"""

# Create a funtion for viewing images in a data batch
def show_25_images(images, labels):
  # Setup the figure
  plt.figure(figsize=(10,10))
  # Loop through 25 (for displaying 25 images)
  for i in range(25):
    # Create subplots (5 rows, 5 columns)
    ax = plt.subplot(5,5,i+1)
    # Display an image
    plt.imshow(images[i])
    # Turn the grid lines off
    plt.axis("off")
    # Add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])

train_images, train_labels = next(train_data.as_numpy_iterator())
show_25_images(train_images, train_labels)

"""# Building a model

There are a few things need to define:
- The input shape (images shape, in the form of Tensors) to the model
- The output shape (image labels, in the form of Tensors) of the model.
- The URL of the model what I want to use
"""

# Setup input shape to the model
input_shape = (None, img_size, img_size, 3) # batch, height, width, colour channels (R,G,B)

# Setup output shape of our model
output_shape = len(unique_breeds)

# Setup model URL from Kaggle
model_url = "https://kaggle.com/models/google/mobilenet-v2/TensorFlow2/130-224-classification/1"

"""Inputs and outputs ready for the model

create a function which:
* Takes the input shape, output shape and the model I have chosen as parameters.
* Define the layers in a Keral model in sequential fashion.
* Compiles the model (says it should be evaluated and improved).
* Builds the model (tells the model the input shape  it will be gettint).
* Returns the model.
"""

# Create a function which builds a Keral model
def create_model(input_shape=input_shape, output_shape=output_shape, model_url=model_url):
  print("Building model with:", model_url)

  # Setup the model layers
  model = tf.keras.Sequential([
    hub.KerasLayer(model_url), # Layer 1 (input layer)
    tf.keras.layers.Dense(units=output_shape, activation="softmax") # Layer 2 (output layer)
  ])

  # Compile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )

  # Build the model
  model.build(input_shape=input_shape)

  return model

model = create_model()
model.summary()

"""## Creating callbacks

Callbacks are helper functions a model can use during training to do such things as save its progress, check its progress or stop training early if a model stops improving.

I will create 2 callbacks, one for TensorBoard which helps track the models progress and another for early stopping which prevents the model from training for too long.

### TensorBoard Callback

To setup a TensorBoard callback, need to do 3 things:
1. Load the TensorBoard notebook extension.
2. Create a TensorBoard callback which is able to save logs to a directory and pass it to the model's `fit()` function.
3. Visualize the models training logs with `%tensorboard` function.
"""

# Commented out IPython magic to ensure Python compatibility.
# Load TensorBoard notebook extension
# %load_ext tensorboard

# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing TensorBoard logs
  logdir = os.path.join("/content/drive/MyDrive/Dog vision/logs",
                        # Make it so the logs get tracked whenever run an expreiment
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

  return tf.keras.callbacks.TensorBoard(logdir)

"""## Early stopping callback


"""

# Create early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                    patience=3)

"""## Training a model (on subset of data)

First model is only going to train on 1000 images, to make sure everything is working.
"""

num_epochs = 100 #@param {type:"slider", min:10, max:100, step:10}

"""Create a function which trains a model

* Create a model using `create_model()`.
* Setup a TensorBoard callback using `create_tensorboard_callback()`.
* Call the `fit()` function on the model passing it the training data, validation data, number of epochs to train for (`num_epochs`) and the callbacks.
* Return the model.
"""

# Build a function to train and return a trained model
def train_model():
  # Create a model
  model = create_model()
  # Create new TensorBoard sepecific to model
  tensorboard = create_tensorboard_callback()
  # Fit the model to the data passing it the callbacks
  model.fit(x=train_data,
            epochs=num_epochs,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])
  # Return the fitted model
  return model

# Fit the model to the data
model = train_model()

"""### Checking the TensorBoard logs

The TensorBoard function (`%tensorboard`) will acces the logs directory I created earlier and visualize its contents
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/Dog\ vision/logs

"""Its look like overfitting because the accuracy is too high on he train data and on the validation is too low

### Making and evaluating predictions using a trained model
"""

val_data

# Make predictions on the validation data (no used to train on)
predictions = model.predict(val_data, verbose=1)
predictions

predictions.shape

len(y_val)

len(unique_breeds)

# First prediction
# this is the softmax algorithm, which breed has the highest chance, the dog will be this breed
index = 3
print(predictions[index])
print(f"Max value (probability of prediction): {np.max(predictions[index])}")
print(f"Sum: {np.sum(predictions[index])}")
print(f"Max index: {np.argmax(predictions[index])}")
print(f"Predicted label: {unique_breeds[np.argmax(predictions[index])]}")

unique_breeds[73]

# Turn prediction probability into their respective label
def get_pred_label(prediction_probabilities):
  return unique_breeds[np.argmax(prediction_probabilities)]

# Get a predicted label based on array of prediction probabilities
pred_labels = [get_pred_label(pred) for pred in predictions]
pred_labels[:10]

"""Now since the validation data is still in a batch dataset, I will have to unbatchify it to make predictions on the validation images and then compare those predictions to the validation labels."""

# Create a function to unbatch a batch dataset
def unbatchify(data):
  images = []
  labels = []

  # Loop through unbatched data
  for image, label in val_data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(label)
  return images, labels

# Unbatchify the validation data
val_images, val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

get_pred_label(val_labels[0])
val_labels[0]

"""I have got ways to get:

* Prediction labels
* Validation labels(truth labels)
* Validation images

I will create a function which:

* Takes an array of prediction probabilities , an array of truth labels and an array of images and integers.
* Convert the prediction probabilities to a predicted label.
* Plot the predicted labels, its predicted probability, the truth label and the target image on a single plot.
"""

# This function visualize one prediction
def plot_pred(prediction_probabilities, labels, images, n=1):
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  # Get the pred label
  pred_label_ = get_pred_label(pred_prob)
  true_label_ = get_pred_label(true_label)

  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the colour of the title depending on if the prediction
  if (pred_label_ == true_label_):
    color = "green"
  else:
    color = "red"

  # Change plot title to be predicted, probability of prediction and truht label
  plt.title('{} {:2.0f}% {}'.format(pred_label_,
                                     np.max(pred_prob)*100,
                                     true_label_),
                                     color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,n=81)

"""This fucntion will:

* Take an input of prediction probabilities array and a ground truth array and an integer
* Find the predcition using get_pred_label()
* Find the top 10:
 * Prediction probabilites indexes
 * Prediction probabilities values
 * Prediction labels
* Plot the top 10 prediction probability values and labels, colouring the true label green
"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label_ = get_pred_label(pred_prob)
  true_label_ = get_pred_label(true_label)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # finr the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]

  # find the top 10 prediction labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color='grey')
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation='vertical')

  # change the colour of true label
  if np.isin(true_label_, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label_)].set_color('green')
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=81)

# Check out a few predictions and their different values
i_multiplier = 0
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range (num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                 labels=val_labels,
                 n=i+i_multiplier)

plt.tight_layout(h_pad=1.0)
plt.show()